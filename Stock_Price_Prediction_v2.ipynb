{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google drive"
      ],
      "metadata": {
        "id": "sOEg1DLiQjhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYus2oszQ8QG",
        "outputId": "9901a544-c1c5-4e6d-a5b8-ba18e18c899b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries\n",
        "Install yfinance using pip to retrieve a wide range of historical stock data from yahoo finance.\n",
        "\n"
      ],
      "metadata": {
        "id": "t6Qo2m3Hs1j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install yfinance\n",
        "!pip install yfinance\n",
        "\n",
        "# Mount google drive in Google Collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "KeV16-Hq7sXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0160064a-d976-4699-ed6c-9254298abdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.40)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.6.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "F1pu3jw398Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "HydzRkyB6wIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to download historica stock data"
      ],
      "metadata": {
        "id": "VVR1kcUF-Tnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def historical_data(tickers, start_date, end_date,\n",
        "                    output_dir='historical_data'):\n",
        "    \"\"\"\n",
        "    Downloads historical stock data for multiple ticker symbols and saves\n",
        "    to CSV files.\n",
        "\n",
        "    Parameters:\n",
        "    - tickers (list): Ticker symbols to download.\n",
        "    - start_date (str): Start date for data in 'YYYY-MM-DD' format.\n",
        "    - end_date (str): End date for data in 'YYYY-MM-DD' format.\n",
        "    - output_dir (str): Directory to save CSV files.\n",
        "\n",
        "    Returns:\n",
        "    - None. Saves CSV files in the specified directory.\n",
        "    \"\"\"\n",
        "    # Create directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate over list of tickers\n",
        "    for ticker in tickers:\n",
        "        print(f'Downloading data for {ticker}...')\n",
        "        # Download data using yfinance\n",
        "        data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        # Define file path\n",
        "        file_path = os.path.join(output_dir,f'{ticker}_historical_data.csv')\n",
        "        # Save data to CSV\n",
        "        data.to_csv(file_path)\n",
        "        print(f'Data for {ticker} saved to {file_path}')\n",
        "        data_dict[ticker] = data\n",
        "\n",
        "    print('Data downloaded and saved to CSV files.')\n",
        "    return\n",
        "\n"
      ],
      "metadata": {
        "id": "piCP7Csl-LpU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define tickers , dates , output directory"
      ],
      "metadata": {
        "id": "UmB2zE3b_gZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List tickers to download data\n",
        "tickers = ['BATS.L', 'LAND.L', 'SVT.L', 'GLEN.L', 'RR.L', 'RTO.L', 'SGE.L',\n",
        "           'CCH.L', 'LGEN.L', 'SN.L']\n",
        "\n",
        "# Indicate start and end date for historical data\n",
        "start_date = '2014-1-01'\n",
        "end_date = '2023-12-31'\n",
        "\n",
        "# Save CSV to directory\n",
        "output_dir = 'FTSE100_data'\n",
        "\n",
        "# Print current working directory\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "# Call the function\n",
        "historical_data(tickers, start_date, end_date, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cZQaeT9_ps9",
        "outputId": "514728d2-476f-4bd2-9a2e-7e1b8af3868a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n",
            "Downloading data for BATS.L...\n",
            "Data for BATS.L saved to FTSE100_data/BATS.L_historical_data.csv\n",
            "Downloading data for LAND.L...\n",
            "Data for LAND.L saved to FTSE100_data/LAND.L_historical_data.csv\n",
            "Downloading data for SVT.L...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for SVT.L saved to FTSE100_data/SVT.L_historical_data.csv\n",
            "Downloading data for GLEN.L...\n",
            "Data for GLEN.L saved to FTSE100_data/GLEN.L_historical_data.csv\n",
            "Downloading data for RR.L...\n",
            "Data for RR.L saved to FTSE100_data/RR.L_historical_data.csv\n",
            "Downloading data for RTO.L...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for RTO.L saved to FTSE100_data/RTO.L_historical_data.csv\n",
            "Downloading data for SGE.L...\n",
            "Data for SGE.L saved to FTSE100_data/SGE.L_historical_data.csv\n",
            "Downloading data for CCH.L...\n",
            "Data for CCH.L saved to FTSE100_data/CCH.L_historical_data.csv\n",
            "Downloading data for LGEN.L...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for LGEN.L saved to FTSE100_data/LGEN.L_historical_data.csv\n",
            "Downloading data for SN.L...\n",
            "Data for SN.L saved to FTSE100_data/SN.L_historical_data.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the data set and Clean up\n",
        "\n",
        "Thiss step involves loading the datasets using pandas  and inspecting the data set structure to understand its contents and columns . Then we clean up the data.\n"
      ],
      "metadata": {
        "id": "lVBNXDkSGr1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "IfmQwLzKIEY8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/historical_data/BATS.L_historical_data.csv',\n",
        "                 header=0)\n",
        "\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.drop(labels=['Open', 'High','Low', 'Close', 'Volume'], axis=1)\n",
        "\n",
        "df = df.rename (columns={'Adj Close': 'BATS'})\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ProMDS7sYL",
        "outputId": "0c60e499-855e-4dfd-cf7e-a4c0c9990001"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date         BATS\n",
            "0    2014-01-02  1721.043335\n",
            "1    2014-01-03  1717.019409\n",
            "2    2014-01-06  1719.970703\n",
            "3    2014-01-07  1709.238647\n",
            "4    2014-01-08  1692.605347\n",
            "...         ...          ...\n",
            "2519 2023-12-21  2228.324951\n",
            "2520 2023-12-22  2233.689697\n",
            "2521 2023-12-27  2229.788086\n",
            "2522 2023-12-28  2226.374023\n",
            "2523 2023-12-29  2239.054443\n",
            "\n",
            "[2524 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/historical_data/CCH.L_historical_data.csv', header=0)\n",
        "\n",
        "\n",
        "df2['Date'] = pd.to_datetime(df2['Date'])\n",
        "df2 = df2.drop(labels=['Open', 'High','Low', 'Close', 'Volume'], axis=1)\n",
        "\n",
        "df2 = df2.rename (columns={'Adj Close': 'CCH.L'})\n",
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpzeVJtPAykn",
        "outputId": "30665626-c4c8-423e-a0bd-566b8458275e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date        CCH.L\n",
            "0    2014-01-02  1241.764038\n",
            "1    2014-01-03  1215.688354\n",
            "2    2014-01-06  1236.126099\n",
            "3    2014-01-07  1268.544312\n",
            "4    2014-01-08  1254.449463\n",
            "...         ...          ...\n",
            "2520 2023-12-21  2209.205322\n",
            "2521 2023-12-22  2211.137207\n",
            "2522 2023-12-27  2210.171143\n",
            "2523 2023-12-28  2209.205322\n",
            "2524 2023-12-29  2226.593018\n",
            "\n",
            "[2525 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merge = pd.merge(df, df2, on='Date', how='inner')\n",
        "print(df_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H4UUtKxB8je",
        "outputId": "39bce28a-5541-42b3-ce1a-bcb711e89fd5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date         BATS        CCH.L\n",
            "0    2014-01-02  1721.043335  1241.764038\n",
            "1    2014-01-03  1717.019409  1215.688354\n",
            "2    2014-01-06  1719.970703  1236.126099\n",
            "3    2014-01-07  1709.238647  1268.544312\n",
            "4    2014-01-08  1692.605347  1254.449463\n",
            "...         ...          ...          ...\n",
            "2519 2023-12-21  2228.324951  2209.205322\n",
            "2520 2023-12-22  2233.689697  2211.137207\n",
            "2521 2023-12-27  2229.788086  2210.171143\n",
            "2522 2023-12-28  2226.374023  2209.205322\n",
            "2523 2023-12-29  2239.054443  2226.593018\n",
            "\n",
            "[2524 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_process_data(tickers, start_date, end_date, output_dir):\n",
        "    \"\"\"\n",
        "    The function downloads historical data for the given tickers from\n",
        "     Yahoo Finance,saves it to CSV files in a specified output directory,\n",
        "     processes each dataset, and returns a merged dataframe.\n",
        "\n",
        "    Parameters:\n",
        "    - tickers: list of stock tickers to download data for\n",
        "    - start_date: start date for historical data in 'YYYY-MM-DD' format\n",
        "    - end_date: end date for historical data in 'YYYY-MM-DD' format\n",
        "    - output_dir: directory to save the CSV files\n",
        "\n",
        "    Returns:\n",
        "    - merged_df: DataFrame with merged data for all tickers\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    processed_dfs = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"Downloading data for {ticker}\")\n",
        "        data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        file_path = os.path.join(output_dir, f\"{ticker}_historical_data.csv\")\n",
        "        data.to_csv(file_path)\n",
        "        print(f\"Data for {ticker} saved to {file_path}\")\n",
        "\n",
        "        # Process the data\n",
        "        df = pd.read_csv(file_path, header=0)\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.drop(labels=['Open', 'High', 'Low', 'Close', 'Volume'], axis=1)\n",
        "        df = df.rename(columns={'Adj Close': ticker.split('.')[0]})\n",
        "\n",
        "        processed_dfs.append(df)\n",
        "\n",
        "    # Merge all dataframes on 'Date'\n",
        "    merged_df = processed_dfs[0]\n",
        "    for df in processed_dfs[1:]:\n",
        "        merged_df = pd.merge(merged_df, df, on='Date', how='inner')\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# List tickers to download data\n",
        "tickers = ['BATS.L', 'CCH.L', 'LAND.L', 'SVT.L', 'GLEN.L', 'RR.L', 'RTO.L',\n",
        "           'SGE.L', 'LGEN.L', 'SN.L']\n",
        "\n",
        "# Indicate start and end date for historical data\n",
        "start_date = '2014-01-01'\n",
        "end_date = '2023-12-31'\n",
        "\n",
        "# Save CSV to directory\n",
        "output_dir = 'historical_data'\n",
        "\n",
        "# Call the function and capture the merged dataframe\n",
        "merged_df = download_and_process_data(tickers, start_date, end_date, output_dir)\n",
        "\n",
        "# Display the merged dataframe\n",
        "print(merged_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmBa_pgOX-Ha",
        "outputId": "41d99984-8ac5-42ea-9577-ace51ee2f062"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for BATS.L\n",
            "Data for BATS.L saved to historical_data/BATS.L_historical_data.csv\n",
            "Downloading data for CCH.L\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for CCH.L saved to historical_data/CCH.L_historical_data.csv\n",
            "Downloading data for LAND.L\n",
            "Data for LAND.L saved to historical_data/LAND.L_historical_data.csv\n",
            "Downloading data for SVT.L\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for SVT.L saved to historical_data/SVT.L_historical_data.csv\n",
            "Downloading data for GLEN.L\n",
            "Data for GLEN.L saved to historical_data/GLEN.L_historical_data.csv\n",
            "Downloading data for RR.L\n",
            "Data for RR.L saved to historical_data/RR.L_historical_data.csv\n",
            "Downloading data for RTO.L\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for RTO.L saved to historical_data/RTO.L_historical_data.csv\n",
            "Downloading data for SGE.L\n",
            "Data for SGE.L saved to historical_data/SGE.L_historical_data.csv\n",
            "Downloading data for LGEN.L\n",
            "Data for LGEN.L saved to historical_data/LGEN.L_historical_data.csv\n",
            "Downloading data for SN.L\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for SN.L saved to historical_data/SN.L_historical_data.csv\n",
            "           Date         BATS          CCH        LAND          SVT  \\\n",
            "0    2014-01-02  1721.043335  1241.764038  645.337585  1098.838989   \n",
            "1    2014-01-03  1717.019409  1215.688354  652.825867  1109.217285   \n",
            "2    2014-01-06  1719.970703  1236.126099  654.527710  1105.325317   \n",
            "3    2014-01-07  1709.238647  1268.544312  664.738525  1081.324829   \n",
            "4    2014-01-08  1692.605347  1254.449463  666.100098  1061.864746   \n",
            "...         ...          ...          ...         ...          ...   \n",
            "2511 2023-12-21  2228.324951  2209.205322  696.267212  2551.663574   \n",
            "2512 2023-12-22  2233.689697  2211.137207  695.300171  2541.957764   \n",
            "2513 2023-12-27  2229.788086  2210.171143  699.748535  2532.251953   \n",
            "2514 2023-12-28  2226.374023  2209.205322  692.785950  2516.722656   \n",
            "2515 2023-12-29  2239.054443  2226.593018  681.568176  2503.134277   \n",
            "\n",
            "            GLEN          RR         RTO          SGE        LGEN           SN  \n",
            "0     208.173996  392.776184   99.460045   303.074066  111.717773   679.736206  \n",
            "1     206.162643  394.327393  101.598061   308.149689  111.667603   681.720215  \n",
            "2     205.358063  399.911804  101.940117   304.150726  113.323006   688.465881  \n",
            "3     207.067780  399.911804  100.828369   302.689667  114.828003   686.481812  \n",
            "4     211.056931  398.981171  101.341469   305.688812  113.925018   689.259460  \n",
            "...          ...         ...         ...          ...         ...          ...  \n",
            "2511  463.648468  299.799988  434.418823  1159.361572  233.970001  1047.291260  \n",
            "2512  459.852112  302.399994  427.803802  1158.870361  235.381729  1050.224854  \n",
            "2513  465.176849  298.799988  438.960480  1161.817993  236.511108  1047.780151  \n",
            "2514  463.500519  298.100006  437.973175  1160.835449  236.511108  1052.669434  \n",
            "2515  465.521973  299.700012  435.208679  1151.992920  236.322876  1054.625244  \n",
            "\n",
            "[2516 rows x 11 columns]\n"
          ]
        }
      ]
    }
  ]
}